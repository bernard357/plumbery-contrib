---

information:
  - "Centralised logging with Elasticsearch, Logstash, and Kibana"

parameters:

  locationId:
    information:
      - "the target data centre for this deployment"
    type: locations.list
    default: AU10

  domainName:
    information:
      - "the name of the network domain to be deployed"
    type: str
    default: ELKFox

  networkName:
    information:
      - "the name of the Ethernet VLAN to be deployed"
    type: str
    default: ELKNetwork

  nodeName:
    information:
      - "the name of the main node to be deployed"
    type: str
    default: logstashServer

links:
  documentation: https://github.com/DimensionDataCBUSydney/plumbery-contrib/tree/master/logging/elasticsearch-kibana-logstash
  credit: http://www.exonet3i.net/evernote/How%20To%20Install%20Elasticsearch,%20Logstash,%20and%20K%20%5B3%5D.html

defaults:

  # the same network domain is used at various facilities
  #
  domain:
    name: "{{ parameter.domainName }}"
    description: "Demonstration of Elasticsearch, Logstash and Kibana"
    ipv4: auto

  # the same ethernet configuration is used at various facilities
  #
  ethernet:
    name: "{{ parameter.networkName }}"
    subnet: 10.0.0.0

  # default settings for a remote logger
  #
  logstashForwarder:

    description: "#logstash-forwarder #ubuntu"

    information:
      - "a remote server with logstash-forwarder"

    appliance: 'Ubuntu 14'
    cpu: 2
    memory: 4

    glue:
      - internet 22

    monitoring: essentials

    cloud-config:

      hostname: "{{ node.name }}"

      packages:
        - ntp

      write_files:

        - path: /root/hosts.awk
          content: |
            #!/usr/bin/awk -f
            /^{{ node.private }}/ {next}
            /^{{ node.ipv6 }}/ {next}
            /^{{ {{ parameter.locationId }}::{{ parameter.nodeName }}.ipv6 }}/ {next}
            {print}
            END {
             print "{{ node.private }}    {{ node.name }}"
             print "{{ node.ipv6 }}    {{ node.name }}"
             print "{{ {{ parameter.locationId }}::{{ parameter.nodeName }}.ipv6 }}    {{ parameter.nodeName }}"
            }

        - path: /root/logstash-forwarder.conf
          content: |
            {
              "network": {
                "servers": [ "{{ {{ parameter.locationId }}::{{ parameter.nodeName }}.ipv6 }}:5000" ],
                "timeout": 15,
                "ssl ca": "/etc/logstash-pki/logstash-forwarder.crt"
              },

              "files": [
                {
                  "paths": [
                    "/var/log/syslog",
                    "/var/log/auth.log"
                   ],
                  "fields": { "type": "syslog" }
                }
              ]
            }

      runcmd:

        - echo "===== Handling ubuntu identity"
        - cp -n /etc/ssh/ssh_host_rsa_key /home/ubuntu/.ssh/id_rsa
        - cp -n /etc/ssh/ssh_host_rsa_key.pub /home/ubuntu/.ssh/id_rsa.pub
        - chown ubuntu:ubuntu /home/ubuntu/.ssh/*
        - sed -i "/StrictHostKeyChecking/s/^.*$/    StrictHostKeyChecking no/" /etc/ssh/ssh_config

        - echo "===== Updating /etc/hosts"
        - cp -n /etc/hosts /etc/hosts.original
        - awk -f /root/hosts.awk /etc/hosts >/etc/hosts.new && mv /etc/hosts.new /etc/hosts

        - echo "===== Installing logstash-forwarder"
        - cd /root
        - wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
        - echo "deb http://packages.elastic.co/logstashforwarder/debian stable main" | sudo tee /etc/apt/sources.list.d/logstashforwarder.list
        - apt-get update
        - apt-get install logstash-forwarder

        - echo "===== Securing logstash-forwarder"
        - sleep 1m
        - mkdir /etc/logstash-pki
        - rsync -zhave "ssh -i /home/ubuntu/.ssh/id_rsa" ubuntu@{{ parameter.nodeName }}:/var/rsync/logstash-forwarder.crt /etc/logstash-pki

        - echo "===== Configuring logstash"
        - cp -n /etc/logstash-forwarder.conf /etc/logstash-forwarder.conf.origin
        - cp /root/logstash-forwarder.conf /etc/logstash-forwarder.conf
        - service logstash-forwarder restart


  cloud-config:

    ssh_keys:
      rsa_private: |
        {{ rsa_private.key }}
      rsa_public: "{{ rsa_public.key }}"

    users:
      - default

      - name: ubuntu
        sudo: 'ALL=(ALL) NOPASSWD:ALL'
        ssh-authorized-keys:
          - "{{ rsa_public.key }}"
          - "{{ rsa_public.local }}"

      - name: root
        sudo: 'ALL=(ALL) NOPASSWD:ALL'
        ssh-authorized-keys:
          - "{{ rsa_public.key }}"
          - "{{ rsa_public.local }}"

    disable_root: false
    ssh_pwauth: false

---

locationId: "{{ parameter.locationId }}"

blueprints:

  - logstash:

      ethernet:
        name: "logstash"
        accept:
          - "AP5::{{ parameter.networkName }}"

      nodes:

        - "{{ parameter.nodeName }}":  # a combo of logstash, elasticsearch, and kibana

            description: "#logstash #elasticsearch #dashboard #kibana #ubuntu"

            information:
              - "a web dashboard to visualize logs:"
              - "http://{{ node.public }}"
              - "authenticate with 'dashboard' and '{{ dashboard.secret }}'"
              - "troubleshoot with:"
              - "$ ssh ubuntu@{{ node.public }}"
              - "check the feeding of elasticsearch with:"
              - "$ curl 'http://localhost:9200/_cat/indices?v'"
              - "validate the configuration of logstash with:"
              - "$ service logstash configtest"

            appliance: 'Ubuntu 14'
            cpu: 2
            memory: 4

            disks:
              - 1 500 standard

            glue:
              - internet 22 80

            monitoring: essentials

            cloud-config:

              hostname: "{{ node.name }}"

              bootcmd:

                # remove apache
                - apt-get remove apache2 -y
                - apt-get autoremove -y

                # automate acceptance of oracle licence
                - echo "oracle-java8-installer shared/accepted-oracle-license-v1-1 select true" | sudo debconf-set-selections
                - echo "oracle-java8-installer shared/accepted-oracle-license-v1-1 seen true" | sudo debconf-set-selections

              apt_sources:
                - source: "ppa:webupd8team/java"

              packages:
                - ntp
                - oracle-java8-installer
                - nginx
                - apache2-utils
                - python-pip

              write_files:

                - path: /root/hosts.awk
                  content: |
                    #!/usr/bin/awk -f
                    /^{{ node.private }}/ {next}
                    /^{{ node.ipv6 }}/ {next}
                    /^{{ logstash-{{ parameter.locationId }}.ipv6 }}/ {next}
                    /^{{ dd-ap::AP5::logstash-AP5.ipv6 }}/ {next}
                    {print}
                    END {
                     print "{{ node.private }}    {{ node.name }}"
                     print "{{ node.ipv6 }}    {{ node.name }}"
                     print "{{ logstash-{{ parameter.locationId }}.ipv6 }}    logstash-{{ parameter.locationId }}"
                     print "{{ dd-ap::AP5::logstash-AP5.ipv6 }}    logstash-AP5"
                    }

                - path: /root/nginx-sites-available-default
                  content: |
                    server {
                        listen 80;

                        server_name {{ node.public }};

                        auth_basic "Restricted Access";
                        auth_basic_user_file /etc/nginx/htpasswd.users;

                        location / {
                            proxy_pass http://localhost:5601;
                            proxy_http_version 1.1;
                            proxy_set_header Upgrade $http_upgrade;
                            proxy_set_header Connection 'upgrade';
                            proxy_set_header Host $host;
                            proxy_cache_bypass $http_upgrade;
                        }
                    }

                - path: /root/logstash-conf.d-01-lumberjack-input.conf
                  content: |
                    input {
                      lumberjack {
                        port => 5000
                        type => "logs"
                        ssl_certificate => "/etc/logstash-pki/logstash-forwarder.crt"
                        ssl_key => "/etc/logstash-pki/logstash-forwarder.key"
                      }
                    }

                - path: /root/logstash-conf.d-10-syslog.conf
                  content: |
                    filter {
                      if [type] == "syslog" {
                        grok {
                          match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
                          add_field => [ "received_at", "%{@timestamp}" ]
                          add_field => [ "received_from", "%{host}" ]
                        }
                        syslog_pri { }
                        date {
                          match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
                        }
                      }
                    }

                - path: /root/logstash-conf.d-30-lumberjack-output.conf
                  content: |
                    output {
                      elasticsearch { hosts => localhost }
                      stdout { codec => rubydebug }
                    }

                - path: /root/etc-cron.d-elasticsearch_curator
                  content: |
                    @midnight     root        curator delete --older-than 120 >> /var/log/curator.log 2>&1

              runcmd:

                - echo "===== Growing LVM with added disk"
                - pvcreate /dev/sdb
                - vgextend rootvol00 /dev/sdb
                - lvextend -l +100%FREE /dev/mapper/rootvol00-rootlvol00
                - resize2fs /dev/mapper/rootvol00-rootlvol00

                - echo "===== Handling ubuntu identity"
                - cp -n /etc/ssh/ssh_host_rsa_key /home/ubuntu/.ssh/id_rsa
                - cp -n /etc/ssh/ssh_host_rsa_key.pub /home/ubuntu/.ssh/id_rsa.pub
                - chown ubuntu:ubuntu /home/ubuntu/.ssh/*
                - sed -i "/StrictHostKeyChecking/s/^.*$/    StrictHostKeyChecking no/" /etc/ssh/ssh_config

                - echo "===== Updating /etc/hosts"
                - cp -n /etc/hosts /etc/hosts.original
                - awk -f /root/hosts.awk /etc/hosts >/etc/hosts.new && mv /etc/hosts.new /etc/hosts

                - echo "===== Installing logstash, elasticsearch, kibana"
                - cd /root
                - wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
                - echo "deb http://packages.elastic.co/elasticsearch/2.x/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.x.list
                - echo "deb http://packages.elastic.co/kibana/4.4/debian stable main" | sudo tee -a /etc/apt/sources.list.d/kibana-4.4.x.list
                - echo "deb http://packages.elastic.co/logstash/2.2/debian stable main" | sudo tee -a /etc/apt/sources.list.d/logstash-2.2.x.list
                - apt-get update
                - apt-get install logstash elasticsearch kibana
                - update-rc.d elasticsearch defaults 95 10
                - service elasticsearch start
                - update-rc.d kibana defaults 96 9
                - service kibana start

                - echo "===== Securing logstash"
                - cp -n /etc/ssl/openssl.cnf /etc/ssl/openssl.cnf.origin
                - sed -i "/# subjectAltName=email:copy/s/^.*$/subjectAltName = IP:{{ node.ipv6 }}/" /etc/ssl/openssl.cnf
                - mkdir /etc/logstash-pki
                - cd /etc/logstash-pki
                - openssl req -config /etc/ssl/openssl.cnf -x509 -days 3650 -batch -nodes -newkey rsa:2048 -keyout logstash-forwarder.key -out logstash-forwarder.crt
                - openssl x509 -in logstash-forwarder.crt -text -noout

                - echo "===== Sharing certificate for remote access"
                - mkdir /var/rsync
                - cp /etc/logstash-pki/logstash-forwarder.crt /var/rsync
                - chown -R ubuntu:ubuntu /var/rsync

                - echo "===== Configuring logstash"
                - cp /root/logstash-conf.d-01-lumberjack-input.conf /etc/logstash/conf.d/01-lumberjack-input.conf
                - cp /root/logstash-conf.d-10-syslog.conf /etc/logstash/conf.d/10-syslog.conf
                - cp /root/logstash-conf.d-30-lumberjack-output.conf /etc/logstash/conf.d/30-lumberjack-output.conf
                - service logstash restart

                - echo "===== Securing web access to Kibana"
                - cp -n /etc/nginx/sites-available/default /etc/nginx/sites-available/default.original
                - cp /root/nginx-sites-available-default /etc/nginx/sites-available/default
                - htpasswd -cb /etc/nginx/htpasswd.users dashboard {{ dashboard.secret }}
                - service nginx restart

                - echo "===== Installing Curator to purge old logs"
                - pip install elasticsearch-curator
                - cp /root/etc-cron.d-elasticsearch_curator /etc/cron.d/elasticsearch_curator


        - "logstash-{{ parameter.locationId }}":
            default: logstashForwarder

---

locationId: AP5

blueprints:

  - bees:

      ethernet:
        accept:
          - "{{ parameter.locationId }}::{{ parameter.networkName }}"

      nodes:
        - logstash-AP5:
            default: logstashForwarder
